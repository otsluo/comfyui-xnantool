import os
import json
import torch
import numpy as np
from PIL import Image, ImageDraw
import cv2
import requests
from io import BytesIO
import tempfile

# å°è¯•å¯¼å…¥SAMï¼Œå¦‚æœä¸å­˜åœ¨åˆ™æä¾›å®‰è£…æç¤º
try:
    from segment_anything import SamPredictor, SamAutomaticMaskGenerator, sam_model_registry
    sam_available = True
except ImportError:
    sam_available = False
    print("âš ï¸ SAMåº“æœªå®‰è£…ï¼Œå°†æä¾›å®‰è£…æç¤º")

# æ”¯æŒçš„SAMæ¨¡å‹ç±»å‹
supported_sam_models = [
    ("vit_h", "SAM ViT-H (å¤§å‹)", "sam_vit_h_4b8939.pth"),
    ("vit_l", "SAM ViT-L (ä¸­å‹)", "sam_vit_l_0b3195.pth"),
    ("vit_b", "SAM ViT-B (å°å‹)", "sam_vit_b_01ec64.pth"),
]

# SAMæ¨¡å‹ä¸‹è½½é“¾æ¥
sam_model_urls = {
    "sam_vit_h_4b8939.pth": "https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth",
    "sam_vit_l_0b3195.pth": "https://dl.fbaipublicfiles.com/segment_anything/sam_vit_l_0b3195.pth",
    "sam_vit_b_01ec64.pth": "https://dl.fbaipublicfiles.com/segment_anything/sam_vit_b_01ec64.pth",
}

# åŠ è½½SAMé…ç½®
def load_sam_config():
    config_path = os.path.join(os.path.dirname(__file__), 'sam_config.json')
    try:
        with open(config_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    except:
        # é»˜è®¤é…ç½®
        return {
            "default_model_type": "vit_b",
            "model_dir": "models/sam",
            "points_per_side": 32,
            "pred_iou_thresh": 0.86,
            "stability_score_thresh": 0.92,
            "crop_n_layers": 1,
            "crop_n_points_downscale_factor": 2,
            "min_mask_region_area": 100,
            "custom_models": []
        }

# ä¿å­˜SAMé…ç½®
def save_sam_config(config: dict) -> bool:
    config_path = os.path.join(os.path.dirname(__file__), 'sam_config.json')
    try:
        with open(config_path, 'w', encoding='utf-8') as f:
            json.dump(config, f, ensure_ascii=False, indent=2)
        return True
    except Exception as e:
        print(f"ä¿å­˜SAMé…ç½®å¤±è´¥: {e}")
        return False

# ä¸‹è½½SAMæ¨¡å‹
def download_sam_model(model_name, save_dir):
    """ä¸‹è½½SAMé¢„è®­ç»ƒæ¨¡å‹"""
    if not sam_available:
        raise ImportError("SAMåº“æœªå®‰è£…ï¼Œè¯·å…ˆå®‰è£…: pip install git+https://github.com/facebookresearch/segment-anything.git")
    
    # ç¡®ä¿ä¿å­˜ç›®å½•å­˜åœ¨
    os.makedirs(save_dir, exist_ok=True)
    
    # æ£€æŸ¥æ¨¡å‹æ˜¯å¦å·²å­˜åœ¨
    save_path = os.path.join(save_dir, model_name)
    if os.path.exists(save_path):
        print(f"âœ… æ¨¡å‹ {model_name} å·²å­˜åœ¨ï¼Œè·³è¿‡ä¸‹è½½")
        return save_path
    
    # è·å–ä¸‹è½½é“¾æ¥
    if model_name not in sam_model_urls:
        raise ValueError(f"æœªçŸ¥çš„SAMæ¨¡å‹: {model_name}")
    
    url = sam_model_urls[model_name]
    print(f"ğŸ“¥ å¼€å§‹ä¸‹è½½SAMæ¨¡å‹: {model_name}")
    print(f"ğŸ”— ä¸‹è½½é“¾æ¥: {url}")
    print("â³ æ¨¡å‹è¾ƒå¤§ï¼ˆçº¦2-3GBï¼‰ï¼Œä¸‹è½½å¯èƒ½éœ€è¦ä¸€äº›æ—¶é—´...")
    
    try:
        # å‘é€è¯·æ±‚
        response = requests.get(url, stream=True)
        response.raise_for_status()
        
        # ä¿å­˜æ–‡ä»¶
        total_size = int(response.headers.get('content-length', 0))
        downloaded_size = 0
        
        with open(save_path, 'wb') as f:
            for chunk in response.iter_content(chunk_size=8192):
                if chunk:
                    f.write(chunk)
                    downloaded_size += len(chunk)
                    # æ˜¾ç¤ºä¸‹è½½è¿›åº¦
                    if total_size > 0:
                        progress = downloaded_size / total_size * 100
                        print(f"ğŸ“Š ä¸‹è½½è¿›åº¦: {progress:.1f}%", end='\r')
        
        print("\nâœ… SAMæ¨¡å‹ä¸‹è½½å®Œæˆï¼")
        return save_path
    except Exception as e:
        # ä¸‹è½½å¤±è´¥ï¼Œåˆ é™¤éƒ¨åˆ†æ–‡ä»¶
        if os.path.exists(save_path):
            os.remove(save_path)
        raise Exception(f"ä¸‹è½½SAMæ¨¡å‹å¤±è´¥: {str(e)}")

class SamModelLoader:
    """SAMæ¨¡å‹åŠ è½½å™¨èŠ‚ç‚¹ - åŠ è½½å’Œé…ç½®SAMæ¨¡å‹"""
    def __init__(self):
        self.models_cache = {}        
        self.config = load_sam_config()
        # ç¡®ä¿æ¨¡å‹ç›®å½•å­˜åœ¨
        os.makedirs(self.config["model_dir"], exist_ok=True)
    
    @classmethod
    def INPUT_TYPES(cls):
        config = load_sam_config()
        
        # æ„å»ºæ¨¡å‹ç±»å‹é€‰æ‹©åˆ—è¡¨
        model_types = [model[0] for model in supported_sam_models]
        model_labels = {model[0]: model[1] for model in supported_sam_models}
        
        return {
            "required": {
                "model_type": (model_types, {
                    "default": config["default_model_type"],
                    "labels": model_labels,
                    "label": "æ¨¡å‹ç±»å‹",
                    "description": "é€‰æ‹©SAMæ¨¡å‹ç±»å‹"
                }),
                "auto_download": ("BOOLEAN", {
                    "default": True,
                    "label": "è‡ªåŠ¨ä¸‹è½½",
                    "description": "æ¨¡å‹ä¸å­˜åœ¨æ—¶è‡ªåŠ¨ä¸‹è½½"
                }),
            },
            "optional": {
                "use_cache": ("BOOLEAN", {
                    "default": True,
                    "label": "ä½¿ç”¨ç¼“å­˜",
                    "description": "æ˜¯å¦ç¼“å­˜å·²åŠ è½½çš„æ¨¡å‹"
                })
            }
        }
    
    RETURN_TYPES = ("SAM_MODEL", "STRING")
    RETURN_NAMES = ("model", "model_info")
    FUNCTION = "load_model"
    CATEGORY = "XnanTool/SAM"
    
    def load_model(self, model_type, auto_download=True, use_cache=True):
        """åŠ è½½SAMæ¨¡å‹"""
        # æ£€æŸ¥SAMåº“æ˜¯å¦å®‰è£…
        if not sam_available:
            raise ImportError("SAMåº“æœªå®‰è£…ï¼Œè¯·å…ˆå®‰è£…: pip install git+https://github.com/facebookresearch/segment-anything.git")
        
        # æŸ¥æ‰¾æ¨¡å‹æ–‡ä»¶åç§°
        model_file = None
        for model in supported_sam_models:
            if model[0] == model_type:
                model_file = model[2]
                break
        
        if not model_file:
            raise ValueError(f"æœªçŸ¥çš„SAMæ¨¡å‹ç±»å‹: {model_type}")
        
        # æ£€æŸ¥ç¼“å­˜
        cache_key = model_type
        if use_cache and cache_key in self.models_cache:
            model = self.models_cache[cache_key]
            model_info = f"å·²ä»ç¼“å­˜åŠ è½½: {model_type}"
            return (model, model_info)
        
        try:
            # æ¨¡å‹è·¯å¾„
            model_path = os.path.join(self.config["model_dir"], model_file)
            
            # å¦‚æœæ¨¡å‹ä¸å­˜åœ¨ä¸”è®¾ç½®äº†è‡ªåŠ¨ä¸‹è½½
            if not os.path.exists(model_path) and auto_download:
                model_path = download_sam_model(model_file, self.config["model_dir"])
            elif not os.path.exists(model_path):
                raise FileNotFoundError(f"SAMæ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}\nè¯·å¯ç”¨è‡ªåŠ¨ä¸‹è½½æˆ–æ‰‹åŠ¨ä¸‹è½½æ¨¡å‹")
            
            # åŠ è½½æ¨¡å‹
            print(f"ğŸš€ åŠ è½½SAMæ¨¡å‹: {model_type} ({model_file})")
            sam = sam_model_registry[model_type](checkpoint=model_path)
            
            # å¦‚æœæœ‰GPUï¼Œç§»è‡³GPU
            if torch.cuda.is_available():
                sam.to(device='cuda')
                device_info = "GPU"
            else:
                device_info = "CPU"
            
            # æ·»åŠ åˆ°ç¼“å­˜
            if use_cache:
                self.models_cache[cache_key] = sam
            
            model_info = f"æˆåŠŸåŠ è½½SAMæ¨¡å‹: {model_type}\nè¿è¡Œè®¾å¤‡: {device_info}\næ¨¡å‹è·¯å¾„: {model_path}"
            return (sam, model_info)
        except Exception as e:
            raise Exception(f"åŠ è½½SAMæ¨¡å‹å¤±è´¥: {str(e)}")

class SamModelLoaderV2:
    """SAMæ¨¡å‹åŠ è½½å™¨V2 - è¯»å–æœ¬åœ°models/samç›®å½•ä¸­çš„æ‰€æœ‰æ¨¡å‹æ–‡ä»¶"""
    def __init__(self):
        self.models_cache = {}
        self.config = load_sam_config()
        # ç¡®ä¿æ¨¡å‹ç›®å½•å­˜åœ¨
        os.makedirs(self.config["model_dir"], exist_ok=True)
    
    @classmethod
    def INPUT_TYPES(cls):
        config = load_sam_config()
        
        # è·å–æ¨¡å‹ç›®å½•ä¸­çš„æ‰€æœ‰.pthæ–‡ä»¶
        model_dir = config["model_dir"]
        local_models = []
        if os.path.exists(model_dir):
            try:
                # åˆ—å‡ºç›®å½•ä¸­çš„æ‰€æœ‰.pthæ–‡ä»¶
                files = [f for f in os.listdir(model_dir) if f.endswith('.pth')]
                # æŒ‰æ–‡ä»¶åæ’åº
                files.sort()
                local_models = files
            except Exception as e:
                print(f"æ‰«ææ¨¡å‹ç›®å½•å¤±è´¥: {e}")
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ¨¡å‹ï¼Œæä¾›ä¸€ä¸ªé»˜è®¤é€‰é¡¹
        if not local_models:
            local_models = ["æ— å¯ç”¨æ¨¡å‹"]
        
        return {
            "required": {
                "model_file": (local_models, {
                    "label": "æ¨¡å‹æ–‡ä»¶",
                    "description": "é€‰æ‹©æœ¬åœ°models/samç›®å½•ä¸­çš„æ¨¡å‹æ–‡ä»¶"
                }),
            },
            "optional": {
                "use_cache": ("BOOLEAN", {
                    "default": True,
                    "label": "ä½¿ç”¨ç¼“å­˜",
                    "description": "æ˜¯å¦ç¼“å­˜å·²åŠ è½½çš„æ¨¡å‹"
                })
            }
        }
    
    RETURN_TYPES = ("SAM_MODEL", "STRING")
    RETURN_NAMES = ("model", "model_info")
    FUNCTION = "load_local_model"
    CATEGORY = "XnanTool/SAM"
    
    def load_local_model(self, model_file, use_cache=True):
        """åŠ è½½æœ¬åœ°SAMæ¨¡å‹æ–‡ä»¶"""
        # æ£€æŸ¥SAMåº“æ˜¯å¦å®‰è£…
        if not sam_available:
            raise ImportError("SAMåº“æœªå®‰è£…ï¼Œè¯·å…ˆå®‰è£…: pip install git+https://github.com/facebookresearch/segment-anything.git")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰å¯ç”¨æ¨¡å‹
        if model_file == "æ— å¯ç”¨æ¨¡å‹":
            raise FileNotFoundError(f"models/samç›®å½•ä¸­æ²¡æœ‰æ‰¾åˆ°.pthæ¨¡å‹æ–‡ä»¶ï¼Œè¯·å…ˆä¸‹è½½æˆ–æ”¾å…¥æ¨¡å‹æ–‡ä»¶")
        
        # æ£€æŸ¥ç¼“å­˜
        cache_key = model_file
        if use_cache and cache_key in self.models_cache:
            model = self.models_cache[cache_key]
            model_info = f"å·²ä»ç¼“å­˜åŠ è½½: {model_file}"
            return (model, model_info)
        
        try:
            # æ¨¡å‹è·¯å¾„
            model_path = os.path.join(self.config["model_dir"], model_file)
            
            # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            if not os.path.exists(model_path):
                raise FileNotFoundError(f"SAMæ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
            
            # å°è¯•ç¡®å®šæ¨¡å‹ç±»å‹
            model_type = self._infer_model_type(model_file)
            
            # åŠ è½½æ¨¡å‹
            print(f"ğŸš€ åŠ è½½æœ¬åœ°SAMæ¨¡å‹: {model_file}")
            sam = sam_model_registry[model_type](checkpoint=model_path)
            
            # å¦‚æœæœ‰GPUï¼Œç§»è‡³GPU
            if torch.cuda.is_available():
                sam.to(device='cuda')
                device_info = "GPU"
            else:
                device_info = "CPU"
            
            # æ·»åŠ åˆ°ç¼“å­˜
            if use_cache:
                self.models_cache[cache_key] = sam
            
            model_info = f"æˆåŠŸåŠ è½½æœ¬åœ°SAMæ¨¡å‹: {model_file}\nè¿è¡Œè®¾å¤‡: {device_info}\næ¨¡å‹è·¯å¾„: {model_path}\næ¨¡å‹ç±»å‹: {model_type}"
            return (sam, model_info)
        except Exception as e:
            raise Exception(f"åŠ è½½æœ¬åœ°SAMæ¨¡å‹å¤±è´¥: {str(e)}")
    
    def _infer_model_type(self, model_file):
        """ä»æ–‡ä»¶åæ¨æ–­æ¨¡å‹ç±»å‹"""
        # æ£€æŸ¥å·²çŸ¥çš„æ¨¡å‹æ–‡ä»¶åæ¨¡å¼
        if "vit_h" in model_file.lower():
            return "vit_h"
        elif "vit_l" in model_file.lower():
            return "vit_l"
        elif "vit_b" in model_file.lower():
            return "vit_b"
        else:
            # é»˜è®¤è¿”å›vit_bï¼Œå¦‚æœä¸ç¡®å®š
            print(f"è­¦å‘Š: æ— æ³•ä»æ–‡ä»¶å{model_file}ç¡®å®šæ¨¡å‹ç±»å‹ï¼Œé»˜è®¤ä½¿ç”¨vit_b")
            return "vit_b"


class SamModelLoaderCustomPath:
    """SAMæ¨¡å‹åŠ è½½å™¨(è‡ªå®šä¹‰è·¯å¾„) - æ”¯æŒç›´æ¥æŒ‡å®šæœ¬åœ°æ¨¡å‹æ–‡ä»¶çš„å®Œæ•´è·¯å¾„"""
    def __init__(self):
        self.models_cache = {}
        self.config = load_sam_config()
    
    @classmethod
    def INPUT_TYPES(cls):
        config = load_sam_config()
        
        return {
            "required": {
                "custom_model_path": ("STRING", {
                    "default": "",
                    "label": "æ¨¡å‹å®Œæ•´è·¯å¾„",
                    "description": "ç›´æ¥è¾“å…¥æœ¬åœ°SAMæ¨¡å‹æ–‡ä»¶çš„å®Œæ•´è·¯å¾„(.pthæ ¼å¼)"
                }),
            },
            "optional": {
                "use_cache": ("BOOLEAN", {
                    "default": True,
                    "label": "ä½¿ç”¨ç¼“å­˜",
                    "description": "æ˜¯å¦ç¼“å­˜å·²åŠ è½½çš„æ¨¡å‹"
                })
            }
        }
    
    RETURN_TYPES = ("SAM_MODEL", "STRING")
    RETURN_NAMES = ("model", "model_info")
    FUNCTION = "load_custom_path_model"
    CATEGORY = "XnanTool/SAM"
    
    def load_custom_path_model(self, custom_model_path, use_cache=True):
        """ä»è‡ªå®šä¹‰è·¯å¾„åŠ è½½SAMæ¨¡å‹"""
        # éªŒè¯è·¯å¾„ä¸ä¸ºç©º
        if not custom_model_path:
            raise Exception("è¯·è¾“å…¥æœ‰æ•ˆçš„æ¨¡å‹æ–‡ä»¶è·¯å¾„")
        
        # ç”Ÿæˆç¼“å­˜é”®
        cache_key = f"custom_path_{os.path.basename(custom_model_path)}"
        
        # æ£€æŸ¥ç¼“å­˜
        if use_cache and cache_key in self.models_cache:
            model = self.models_cache[cache_key]
            model_info = f"å·²ä»ç¼“å­˜åŠ è½½è‡ªå®šä¹‰è·¯å¾„æ¨¡å‹: {custom_model_path}"
            return (model, model_info)
        
        try:
            # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            if not os.path.exists(custom_model_path):
                raise FileNotFoundError(f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {custom_model_path}")
            
            # æ£€æŸ¥æ–‡ä»¶å¤§å°ï¼Œå¤ªå°å¯èƒ½æ˜¯æŸåçš„
            if os.path.getsize(custom_model_path) < 1024 * 1024:  # å°äº1MB
                raise ValueError(f"æ¨¡å‹æ–‡ä»¶å¯èƒ½æŸåï¼Œå¤§å°è¿‡å°: {custom_model_path}")
            
            # å°è¯•ç¡®å®šæ¨¡å‹ç±»å‹
            model_type = self._infer_model_type(os.path.basename(custom_model_path))
            
            # åŠ è½½æ¨¡å‹
            print(f"ğŸš€ åŠ è½½è‡ªå®šä¹‰è·¯å¾„SAMæ¨¡å‹: {custom_model_path}")
            sam = sam_model_registry[model_type](checkpoint=custom_model_path)
            
            # å¦‚æœæœ‰GPUï¼Œç§»è‡³GPU
            if torch.cuda.is_available():
                sam.to(device='cuda')
                device_info = "GPU"
            else:
                device_info = "CPU"
            
            # æ·»åŠ åˆ°ç¼“å­˜
            if use_cache:
                self.models_cache[cache_key] = sam
            
            model_info = f"æˆåŠŸåŠ è½½è‡ªå®šä¹‰è·¯å¾„SAMæ¨¡å‹: {custom_model_path}\nè¿è¡Œè®¾å¤‡: {device_info}\næ¨¡å‹ç±»å‹: {model_type}"
            return (sam, model_info)
        except Exception as e:
            # æä¾›æ›´è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯
            error_msg = f"åŠ è½½è‡ªå®šä¹‰è·¯å¾„SAMæ¨¡å‹å¤±è´¥: {str(e)}"
            raise Exception(error_msg)
    
    def _infer_model_type(self, model_file):
        """ä»æ–‡ä»¶åæ¨æ–­æ¨¡å‹ç±»å‹"""
        # æ£€æŸ¥å·²çŸ¥çš„æ¨¡å‹æ–‡ä»¶åæ¨¡å¼
        if "vit_h" in model_file.lower():
            return "vit_h"
        elif "vit_l" in model_file.lower():
            return "vit_l"
        elif "vit_b" in model_file.lower():
            return "vit_b"
        else:
            # é»˜è®¤è¿”å›vit_bï¼Œå¦‚æœä¸ç¡®å®š
            print(f"è­¦å‘Š: æ— æ³•ä»æ–‡ä»¶å{model_file}ç¡®å®šæ¨¡å‹ç±»å‹ï¼Œé»˜è®¤ä½¿ç”¨vit_b")
            return "vit_b"


# æ›´æ–°èŠ‚ç‚¹æ˜ å°„
NODE_CLASS_MAPPINGS = {
    "SamModelLoader": SamModelLoader,
    "SamModelLoaderV2": SamModelLoaderV2,
    "SamModelLoaderCustomPath": SamModelLoaderCustomPath,
}

NODE_DISPLAY_NAME_MAPPINGS = {
    "SamModelLoader": "SAMæ¨¡å‹åŠ è½½å™¨ï¼ˆé¢„è®¾ï¼‰-ã€æ–°ã€‘",
    "SamModelLoaderV2": "SAMæ¨¡å‹åŠ è½½å™¨V2 (æœ¬åœ°æ¨¡å‹)-ã€æ–°ã€‘",
    "SamModelLoaderCustomPath": "SAMæ¨¡å‹åŠ è½½å™¨(è‡ªå®šä¹‰è·¯å¾„)-ã€æ–°ã€‘",
}

# ç¡®ä¿æ¨¡å—è¢«æ­£ç¡®å¯¼å…¥
__all__ = ['NODE_CLASS_MAPPINGS', 'NODE_DISPLAY_NAME_MAPPINGS']