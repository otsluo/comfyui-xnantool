import requests
import json
import time
import torch
import numpy as np
from PIL import Image
from io import BytesIO
import os
import base64
import tempfile
import random

# é…ç½®ç›¸å…³å‡½æ•°
def load_config():
    # ä½¿ç”¨é»˜è®¤é…ç½®
    return {
        "default_lora_model": "qiyuanai/TikTok_Xiaohongshu_career_line_beauty_v1",
        "timeout": 720,
        "image_download_timeout": 30,
        "default_prompt": "Career line,with prominent breasts,A very realistic style,high definition photography style,a young woman,long black hair,holding a badminton shuttlecock,standing,outdoors.",
        "default_edit_prompt": "ä¿®æ”¹å›¾ç‰‡ä¸­çš„å†…å®¹",
        "default_negative_prompt": "",
        "default_width": 512,
        "default_height": 512,
        "default_seed": -1,
        "default_steps": 30,
        "default_guidance": 7.5
    }

def save_config(config: dict) -> bool:
    print("é…ç½®ä¿å­˜åŠŸèƒ½å·²ç¦ç”¨ï¼Œä¸å†ä½¿ç”¨modelscope_api_node.jsonæ–‡ä»¶")
    return True

def load_api_token():
    return ""

def save_api_token(token):
    print("Tokenä¿å­˜åŠŸèƒ½å·²ç¦ç”¨ï¼Œä¸å†ä½¿ç”¨.modelscope_api_tokenæ–‡ä»¶")
    return True

def tensor_to_base64_url(image_tensor):
    try:
        if len(image_tensor.shape) == 4:
            image_tensor = image_tensor.squeeze(0)
        
        if image_tensor.max() <= 1.0:
            image_np = (image_tensor.cpu().numpy() * 255).astype(np.uint8)
        else:
            image_np = image_tensor.cpu().numpy().astype(np.uint8)
        
        pil_image = Image.fromarray(image_np)
        
        buffer = BytesIO()
        pil_image.save(buffer, format='JPEG', quality=85)
        img_base64 = base64.b64encode(buffer.getvalue()).decode('utf-8')
        
        return f"data:image/jpeg;base64,{img_base64}"
        
    except Exception as e:
        print(f"å›¾åƒè½¬æ¢å¤±è´¥: {e}")
        raise Exception(f"å›¾åƒæ ¼å¼è½¬æ¢å¤±è´¥: {str(e)}")

# æ”¯æŒçš„åŸºç¡€æ¨¡å‹åˆ—è¡¨
SUPPORTED_TEXT_TO_IMAGE_MODELS = [
    ("Qwen/Qwen-Image", "Qwen-Image"),
    ("black-forest-labs/FLUX.1-schnell", "FLUX.1-schnell"),
    ("stabilityai/stable-diffusion-3-medium-diffusers", "SD3 Medium"),
    ("segmind/Segmind-Vega", "Segmind-Vega"),
    ("stabilityai/stable-diffusion-xl-base-1.0", "SDXL 1.0"),
]

SUPPORTED_IMAGE_EDIT_MODELS = [
    ("Qwen/Qwen-Image-Edit", "Qwen-Image-Edit"),
    ("Qwen/Qwen-Image-Edit-2509", "Qwen-Image-Edit-2509"),
    ("runwayml/stable-diffusion-inpainting", "SD Inpainting"),
]

class modelscopeLoraTextToImageNode:
    """æ”¯æŒå¤šç§åŸºç¡€æ¨¡å‹çš„æ–‡ç”Ÿå›¾èŠ‚ç‚¹ï¼ŒåŒ…å«LoRAæ”¯æŒå’Œæ‰¹æ¬¡ç”ŸæˆåŠŸèƒ½"""
    def __init__(self):
        pass
    
    @classmethod
    def INPUT_TYPES(cls):
        saved_token = load_api_token()
        return {
            "required": {
                "prompt": ("STRING", {
                    "multiline": True,
                    "default": "A beautiful portrait",
                    "label": "æç¤ºè¯",
                    "description": "æè¿°æ‚¨æƒ³è¦ç”Ÿæˆçš„å›¾åƒå†…å®¹",
                    "placeholder": "æè¿°æ‚¨æƒ³è¦ç”Ÿæˆçš„å›¾åƒå†…å®¹"
                }),
                "api_token": ("STRING", {
                    "default": saved_token,
                    "label": "API Token",
                    "description": "modelscope API ä»¤ç‰Œï¼Œç”¨äºè°ƒç”¨æœåŠ¡",
                    "placeholder": "è¯·è¾“å…¥æ‚¨çš„ modelscope API Token"
                }),
                "base_model": ("STRING", {
                    "default": SUPPORTED_TEXT_TO_IMAGE_MODELS[0][0],
                    "options": [model[0] for model in SUPPORTED_TEXT_TO_IMAGE_MODELS],
                    "labels": {model[0]: model[1] for model in SUPPORTED_TEXT_TO_IMAGE_MODELS},
                    "label": "åŸºç¡€æ¨¡å‹"
                }),
                "lora_model": ("STRING", {
                    "default": "qiyuanai/TikTok_Xiaohongshu_career_line_beauty_v1",
                    "label": "LoRAæ¨¡å‹"
                }),
            },
            "optional": {
                "negative_prompt": ("STRING", {
                    "multiline": True,
                    "default": "",
                    "label": "è´Ÿé¢æç¤ºè¯",
                    "placeholder": "æè¿°æ‚¨ä¸æƒ³åœ¨å›¾åƒä¸­å‡ºç°çš„å†…å®¹"
                }),
                "width": ("INT", {
                    "default": 512,
                    "min": 64,
                    "max": 2048,
                    "step": 64,
                    "label": "å®½åº¦"
                }),
                "height": ("INT", {
                    "default": 512,
                    "min": 64,
                    "max": 2048,
                    "step": 64,
                    "label": "é«˜åº¦"
                }),
                "seed": ("INT", {
                    "default": -1,
                    "min": -1,
                    "max": 2147483647,
                    "label": "éšæœºç§å­"
                }),
                "steps": ("INT", {
                    "default": 30,
                    "min": 1,
                    "max": 100,
                    "label": "é‡‡æ ·æ­¥æ•°"
                }),
                "guidance": ("FLOAT", {
                    "default": 7.5,
                    "min": 1.5,
                    "max": 20.0,
                    "step": 0.1,
                    "label": "å¼•å¯¼ç³»æ•°"
                }),
                "lora_weight": ("FLOAT", {
                    "default": 0.8,
                    "min": 0.0,
                    "max": 2.0,
                    "step": 0.1,
                    "label": "LoRAæƒé‡"
                }),
                "batch_size": ("INT", {
                    "default": 1,
                    "min": 1,
                    "max": 8,
                    "label": "æ‰¹æ¬¡å¤§å°",
                    "description": "ä¸€æ¬¡ç”Ÿæˆçš„å›¾ç‰‡æ•°é‡"
                }),
            }
        }
    
    RETURN_TYPES = ("IMAGE",)
    RETURN_NAMES = ("images",)
    FUNCTION = "generate_with_lora"
    CATEGORY = "XnanTool/é­”æ­api"
    
    def generate_with_lora(self, prompt, api_token, base_model, lora_model, batch_size=1, negative_prompt="", 
                          width=512, height=512, seed=-1, steps=30, guidance=7.5, lora_weight=0.8, generate_control="fixed"):
        
        # éªŒè¯API Token
        if not api_token or api_token.strip() == "" or api_token.strip() == "api_token":
            raise Exception("è¯·è¾“å…¥æœ‰æ•ˆçš„API Tokenï¼ˆå½“å‰é…ç½®æ— æ•ˆï¼‰")
        
        # ä¿å­˜API Tokenï¼ˆå¦‚æœæœ‰å˜åŒ–ï¼‰
        saved_token = load_api_token()
        if api_token != saved_token:
            if save_api_token(api_token):
                print("âœ… API Tokenå·²è‡ªåŠ¨ä¿å­˜")
            else:
                print("âš ï¸ API Tokenä¿å­˜å¤±è´¥ï¼Œä½†ä¸å½±å“å½“å‰ä½¿ç”¨")
        
        try:
            # ä¸ºæ¯ä¸ªæ‰¹æ¬¡ç”Ÿæˆä½¿ç”¨ä¸åŒçš„ç§å­
            base_seed = seed if seed != -1 else random.randint(0, 20251003)
            
            # å­˜å‚¨æ‰€æœ‰ç”Ÿæˆçš„å›¾åƒ
            image_tensors = []
            
            # ä¸ºæ¯ä¸ªæ‰¹æ¬¡ç”Ÿæˆå›¾åƒ
            for i in range(batch_size):
                current_seed = base_seed + i if seed != -1 else random.randint(0, 20251003)
                
                # å‡†å¤‡APIè¯·æ±‚å‚æ•°
                url = 'https://api-inference.modelscope.cn/v1/images/generations'
                
                # åŸºç¡€payload
                payload = {
                    'model': base_model,  # ä½¿ç”¨ç”¨æˆ·é€‰æ‹©çš„åŸºç¡€æ¨¡å‹
                    'prompt': prompt,
                    'size': f"{width}x{height}",
                    'steps': steps,
                    'guidance_scale': guidance,
                    'generate_control': generate_control,
                    'seed': current_seed
                }
                
                # ä¿®å¤LoRAå‚æ•°æ ¼å¼ - æŒ‰ç…§å®˜æ–¹æ–‡æ¡£è¦æ±‚
                if lora_model and lora_model.strip() != "":
                    # å•ä¸ªLoRAæ¨¡å‹æ ¼å¼ï¼š{"model_id": weight}
                    payload['loras'] = {lora_model: lora_weight}
                
                # æ·»åŠ å¯é€‰å‚æ•°
                if negative_prompt.strip():
                    payload['negative_prompt'] = negative_prompt
                
                # å‡†å¤‡è¯·æ±‚å¤´ - ç»Ÿä¸€æ ¼å¼
                common_headers = {
                    'Authorization': f'Bearer {api_token}',
                    'Content-Type': 'application/json',
                }
                
                headers = {** common_headers, "X-ModelScope-Async-Mode": "true"}
                
                # å‘é€è¯·æ±‚
                print(f"ğŸ“¤ æ­£åœ¨æäº¤ç¬¬ {i+1}/{batch_size} ä¸ªLoRAå›¾åƒç”Ÿæˆä»»åŠ¡ï¼Œç§å­: {current_seed}")
                submission_response = requests.post(
                    url,
                    data=json.dumps(payload, ensure_ascii=False).encode('utf-8'),
                    headers=headers,
                    timeout=60
                )
                
                # å¤„ç†è¯·æ±‚å“åº”
                if submission_response.status_code != 200:
                    error_detail = submission_response.text
                    print(f"âŒ APIè¯·æ±‚å¤±è´¥è¯¦æƒ…:")
                    print(f"   çŠ¶æ€ç : {submission_response.status_code}")
                    print(f"   å“åº”å†…å®¹: {error_detail}")
                    try:
                        error_json = submission_response.json()
                        if "errors" in error_json:
                            error_message = error_json["errors"].get("message", "æœªçŸ¥é”™è¯¯")
                            error_code = error_json["errors"].get("code", "æœªçŸ¥é”™è¯¯ç ")
                            raise Exception(f"APIè¯·æ±‚å¤±è´¥ [{submission_response.status_code}]: {error_code} - {error_message}")
                    except:
                        pass
                    raise Exception(f"APIè¯·æ±‚å¤±è´¥: {submission_response.status_code}, {error_detail}")
                
                submission_json = submission_response.json()
                
                # å¤„ç†å¼‚æ­¥ä»»åŠ¡
                image_url = None
                if 'task_id' in submission_json:
                    task_id = submission_json['task_id']
                    print(f"ğŸ•’ å·²æäº¤ç¬¬ {i+1} ä¸ªä»»åŠ¡ï¼Œä»»åŠ¡ID: {task_id}ï¼Œå¼€å§‹è½®è¯¢...")
                    poll_start = time.time()
                    max_wait_seconds = 720
                    
                    while True:
                        # æŸ¥è¯¢ä»»åŠ¡çŠ¶æ€ - ä¿®å¤è¯·æ±‚å¤´æ ¼å¼
                        task_resp = requests.get(
                            f"https://api-inference.modelscope.cn/v1/tasks/{task_id}",
                            headers={**common_headers, "X-ModelScope-Task-Type": "image_generation"},
                            timeout=120
                        )
                        
                        if task_resp.status_code != 200:
                            raise Exception(f"ä»»åŠ¡æŸ¥è¯¢å¤±è´¥: {task_resp.status_code}, {task_resp.text}")
                        
                        data = task_resp.json()
                        task_status = data.get("task_status")
                        
                        if task_status == "SUCCEED":
                            if not data.get("output_images") or len(data["output_images"]) == 0:
                                raise Exception("ä»»åŠ¡æˆåŠŸä½†æœªè¿”å›å›¾ç‰‡URL")
                            
                            image_url = data["output_images"][0]
                            print(f"âœ… ç¬¬ {i+1} ä¸ªä»»åŠ¡å®Œæˆï¼Œå¼€å§‹ä¸‹è½½å›¾ç‰‡...")
                            
                            # ä¸‹è½½å›¾ç‰‡
                            img_response = requests.get(image_url, timeout=30)
                            if img_response.status_code != 200:
                                raise Exception(f"å›¾ç‰‡ä¸‹è½½å¤±è´¥: {img_response.status_code}")
                            
                            # å¤„ç†å›¾ç‰‡
                            pil_image = Image.open(BytesIO(img_response.content))
                            if pil_image.mode != 'RGB':
                                pil_image = pil_image.convert('RGB')
                            
                            # è½¬æ¢ä¸ºComfyUIéœ€è¦çš„æ ¼å¼
                            image_np = np.array(pil_image).astype(np.float32) / 255.0
                            image_tensor = torch.from_numpy(image_np)[None,]
                            
                            # æ·»åŠ åˆ°å›¾åƒåˆ—è¡¨
                            image_tensors.append(image_tensor)
                            break
                            
                        elif task_status == "FAILED":
                            error_message = data.get("errors", {}).get("message", "æœªçŸ¥é”™è¯¯")
                            error_code = data.get("errors", {}).get("code", "æœªçŸ¥é”™è¯¯ç ")
                            raise Exception(f"ä»»åŠ¡å¤±è´¥: é”™è¯¯ç  {error_code}, é”™è¯¯ä¿¡æ¯: {error_message}")
                            
                        # æ£€æŸ¥è¶…æ—¶
                        if time.time() - poll_start > max_wait_seconds:
                            raise Exception("ä»»åŠ¡è½®è¯¢è¶…æ—¶ï¼Œè¯·ç¨åé‡è¯•æˆ–é™ä½å¹¶å‘")
                            
                        # æœªå®Œæˆï¼Œç»§ç»­è½®è¯¢
                        time.sleep(5)
                else:
                    raise Exception(f"æœªè¯†åˆ«çš„APIè¿”å›æ ¼å¼: {submission_json}")
            
            # åˆå¹¶æ‰€æœ‰å›¾åƒå¼ é‡
            if len(image_tensors) > 0:
                final_tensor = torch.cat(image_tensors, dim=0)
                print(f"ğŸ‰ æ‰¹æ¬¡å›¾ç‰‡ç”Ÿæˆå®Œæˆï¼å…±ç”Ÿæˆ {len(image_tensors)} å¼ å›¾ç‰‡")
                return (final_tensor,)
            else:
                raise Exception("æœªç”Ÿæˆä»»ä½•å›¾ç‰‡")
        
        except Exception as e:
            print(f"é­”æ­API-LoRAè°ƒç”¨å¤±è´¥: {str(e)}")
            # åˆ›å»ºä¸€ä¸ªçº¢è‰²é”™è¯¯å›¾åƒä½œä¸ºå›é€€
            error_image = Image.new('RGB', (width, height), color='red')
            error_np = np.array(error_image).astype(np.float32) / 255.0
            error_tensor = torch.from_numpy(error_np)[None,]
            # å¦‚æœæ˜¯æ‰¹æ¬¡ç”Ÿæˆï¼Œå¤åˆ¶å›¾åƒä»¥åŒ¹é…æ‰¹æ¬¡å¤§å°
            if batch_size > 1:
                error_tensor = error_tensor.repeat(batch_size, 1, 1, 1)
            return (error_tensor,)

class modelscopeLoraImageEditNode:
    """æ”¯æŒå¤šç§åŸºç¡€æ¨¡å‹çš„å›¾åƒç¼–è¾‘èŠ‚ç‚¹ï¼ŒåŒ…å«LoRAæ”¯æŒ"""
    def __init__(self):
        pass
    
    @classmethod
    def INPUT_TYPES(cls):
        saved_token = load_api_token()
        return {
            "required": {
                "image": ("IMAGE",),
                "prompt": ("STRING", {
                    "multiline": True,
                    "default": "ä¿®æ”¹å›¾ç‰‡ä¸­çš„å†…å®¹",
                    "label": "ç¼–è¾‘æç¤ºè¯",
                    "description": "æè¿°æ‚¨æƒ³è¦å¦‚ä½•ç¼–è¾‘å›¾åƒ",
                    "placeholder": "æè¿°æ‚¨æƒ³è¦å¦‚ä½•ç¼–è¾‘å›¾åƒ"
                }),
                "api_token": ("STRING", {
                    "default": saved_token,
                    "placeholder": "è¯·è¾“å…¥æ‚¨çš„é­”æ­API Token",
                    "label": "APIä»¤ç‰Œ"
                }),
                "base_model": ("STRING", {
                    "default": SUPPORTED_IMAGE_EDIT_MODELS[0][0],
                    "options": [model[0] for model in SUPPORTED_IMAGE_EDIT_MODELS],
                    "labels": {model[0]: model[1] for model in SUPPORTED_IMAGE_EDIT_MODELS},
                    "label": "åŸºç¡€æ¨¡å‹"
                }),
                "lora_model": ("STRING", {
                    "default": "qiyuanai/TikTok_Xiaohongshu_career_line_beauty_v1",
                    "label": "LoRAæ¨¡å‹"
                }),
            },
            "optional": {
                "negative_prompt": ("STRING", {
                    "multiline": True,
                    "default": "",
                    "label": "è´Ÿé¢æç¤ºè¯",
                    "placeholder": "æè¿°æ‚¨ä¸æƒ³åœ¨ç¼–è¾‘åå›¾åƒä¸­å‡ºç°çš„å†…å®¹"
                }),
                "use_custom_size": ("BOOLEAN", {
                    "default": False,
                    "label": "ä½¿ç”¨è‡ªå®šä¹‰å°ºå¯¸",
                    "description": "å¼€å¯æ—¶ä½¿ç”¨è‡ªå®šä¹‰å®½åº¦å’Œé«˜åº¦ï¼Œå…³é—­æ—¶è‡ªåŠ¨è·å–è¾“å…¥å›¾åƒå°ºå¯¸"
                }),
                "width": ("INT", {
                    "default": 512,
                    "min": 64,
                    "max": 1664,
                    "step": 8,
                    "label": "å®½åº¦"
                }),
                "height": ("INT", {
                    "default": 512,
                    "min": 64,
                    "max": 1664,
                    "step": 8,
                    "label": "é«˜åº¦"
                }),
                "seed": ("INT", {
                    "default": -1,
                    "min": -1,
                    "max": 20251003,
                    "label": "éšæœºç§å­"
                }),
                "steps": ("INT", {
                    "default": 30,
                    "min": 1,
                    "max": 100,
                    "label": "é‡‡æ ·æ­¥æ•°"
                }),
                "guidance": ("FLOAT", {
                    "default": 3.5,
                    "min": 1.5,
                    "max": 20.0,
                    "step": 0.1,
                    "label": "å¼•å¯¼ç³»æ•°"
                }),
                "lora_weight": ("FLOAT", {
                    "default": 0.8,
                    "min": 0.0,
                    "max": 2.0,
                    "step": 0.1,
                    "label": "LoRAæƒé‡"
                }),
            }
        }
    
    RETURN_TYPES = ("IMAGE",)
    RETURN_NAMES = ("edited_image",)
    FUNCTION = "edit_with_lora"
    CATEGORY = "XnanTool/é­”æ­api"
    
    def edit_with_lora(self, image, prompt, api_token, base_model, lora_model, negative_prompt="", 
                       use_custom_size=False, width=512, height=512, seed=-1, steps=30, guidance=3.5, lora_weight=0.8, generate_control="fixed"):
        
        # éªŒè¯API Token
        if not api_token or api_token.strip() == "" or api_token.strip() == "api_token":
            raise Exception("è¯·è¾“å…¥æœ‰æ•ˆçš„API Tokenï¼ˆå½“å‰é…ç½®æ— æ•ˆï¼‰")
        
        # ä¿å­˜API Tokenï¼ˆå¦‚æœæœ‰å˜åŒ–ï¼‰
        saved_token = load_api_token()
        if api_token != saved_token:
            if save_api_token(api_token):
                print("âœ… API Tokenå·²è‡ªåŠ¨ä¿å­˜")
            else:
                print("âš ï¸ API Tokenä¿å­˜å¤±è´¥ï¼Œä½†ä¸å½±å“å½“å‰ä½¿ç”¨")
        
        try:
            # ç›´æ¥ä½¿ç”¨base64ç¼–ç æ–¹å¼
            print("ğŸ“¤ ä½¿ç”¨base64ç¼–ç æ–¹å¼ä¸Šä¼ å›¾åƒ...")
            image_data = tensor_to_base64_url(image)
            
            # å‡†å¤‡é€šç”¨è¯·æ±‚å¤´
            common_headers = {
                'Authorization': f'Bearer {api_token}',
                'Content-Type': 'application/json',
            }
            
            payload = {
                'model': base_model,
                'prompt': prompt,
                'image': image_data,
                'generate_control': generate_control
            }
            
            # ä¿®å¤LoRAå‚æ•°æ ¼å¼ - æŒ‰ç…§å®˜æ–¹æ–‡æ¡£è¦æ±‚
            if lora_model and lora_model.strip() != "":
                payload['loras'] = {lora_model: lora_weight}
            
            # æ·»åŠ å¯é€‰å‚æ•°
            if negative_prompt.strip():
                payload['negative_prompt'] = negative_prompt
                print(f"ğŸš« è´Ÿå‘æç¤ºè¯: {negative_prompt}")
            
            # å¤„ç†å›¾åƒå°ºå¯¸
            if use_custom_size:
                if width != 512 or height != 512:
                    size = f"{width}x{height}"
                    payload['size'] = size
                    print(f"ï¿½ ä½¿ç”¨è‡ªå®šä¹‰å›¾åƒå°ºå¯¸: {size}")
            else:
                if len(image.shape) == 4:
                    img = image[0]
                else:
                    img = image
                
                img_height, img_width = img.shape[:2]
                img_width = (img_width // 8) * 8
                img_height = (img_height // 8) * 8
                
                size = f"{img_width}x{img_height}"
                payload['size'] = size
                print(f"ğŸ“ è‡ªåŠ¨è·å–è¾“å…¥å›¾åƒå°ºå¯¸: {size}")
            
            # æ·»åŠ å…¶ä»–å‚æ•°
            if steps != 30:
                payload['steps'] = steps
                print(f"ğŸ”„ é‡‡æ ·æ­¥æ•°: {steps}")
            
            if guidance != 3.5:
                payload['guidance_scale'] = guidance
                print(f"ğŸ§­ å¼•å¯¼ç³»æ•°: {guidance}")
            
            # å¤„ç†ç§å­
            if seed != -1:
                payload['seed'] = seed
                print(f"ğŸ² éšæœºç§å­: {seed}")
            
            model_display_name = next((model[1] for model in SUPPORTED_IMAGE_EDIT_MODELS if model[0] == base_model), base_model)
            print(f"ğŸ”§ ä½¿ç”¨åŸºç¡€æ¨¡å‹: {model_display_name} ({base_model})")
            print(f"ğŸ§© ä½¿ç”¨LoRAæ¨¡å‹: {lora_model}")
            print(f"âš–ï¸ LoRAæƒé‡: {lora_weight}")
            
            # å‘é€è¯·æ±‚
            print("ğŸ“¤ æ­£åœ¨æäº¤LoRAå›¾åƒç¼–è¾‘ä»»åŠ¡...")
            url = 'https://api-inference.modelscope.cn/v1/images/generations'
            headers = {** common_headers, "X-ModelScope-Async-Mode": "true"}
            
            submission_response = requests.post(
                url,
                data=json.dumps(payload, ensure_ascii=False).encode('utf-8'),
                headers=headers,
                timeout=60
            )
            
            # å¤„ç†è¯·æ±‚å“åº”
            if submission_response.status_code != 200:
                error_detail = submission_response.text
                print(f"âŒ APIè¯·æ±‚å¤±è´¥è¯¦æƒ…:")
                print(f"   çŠ¶æ€ç : {submission_response.status_code}")
                print(f"   å“åº”å†…å®¹: {error_detail}")
                try:
                    error_json = submission_response.json()
                    if "errors" in error_json:
                        error_message = error_json["errors"].get("message", "æœªçŸ¥é”™è¯¯")
                        error_code = error_json["errors"].get("code", "æœªçŸ¥é”™è¯¯ç ")
                        raise Exception(f"APIè¯·æ±‚å¤±è´¥ [{submission_response.status_code}]: {error_code} - {error_message}")
                except:
                    pass
                raise Exception(f"APIè¯·æ±‚å¤±è´¥: {submission_response.status_code}, {error_detail}")
            
            submission_json = submission_response.json()
            
            # å¤„ç†å¼‚æ­¥ä»»åŠ¡
            result_image_url = None
            if 'task_id' in submission_json:
                task_id = submission_json['task_id']
                print(f"ğŸ•’ å·²æäº¤ä»»åŠ¡ï¼Œä»»åŠ¡ID: {task_id}ï¼Œå¼€å§‹è½®è¯¢...")
                poll_start = time.time()
                max_wait_seconds = 720
                
                while True:
                    # æŸ¥è¯¢ä»»åŠ¡çŠ¶æ€ - ä¿®å¤è¯·æ±‚å¤´æ ¼å¼
                    task_resp = requests.get(
                        f"https://api-inference.modelscope.cn/v1/tasks/{task_id}",
                        headers={**common_headers, "X-ModelScope-Task-Type": "image_editing"},
                        timeout=120
                    )
                    
                    if task_resp.status_code != 200:
                        raise Exception(f"ä»»åŠ¡æŸ¥è¯¢å¤±è´¥: {task_resp.status_code}, {task_resp.text}")
                    
                    data = task_resp.json()
                    task_status = data.get("task_status")
                    
                    if task_status == "SUCCEED":
                        if not data.get("output_images") or len(data["output_images"]) == 0:
                            raise Exception("ä»»åŠ¡æˆåŠŸä½†æœªè¿”å›å›¾ç‰‡URL")
                        
                        result_image_url = data["output_images"][0]
                        print("âœ… ä»»åŠ¡å®Œæˆï¼Œå¼€å§‹ä¸‹è½½ç¼–è¾‘åçš„å›¾ç‰‡...")
                        break
                        
                    elif task_status == "FAILED":
                        error_message = data.get("errors", {}).get("message", "æœªçŸ¥é”™è¯¯")
                        error_code = data.get("errors", {}).get("code", "æœªçŸ¥é”™è¯¯ç ")
                        raise Exception(f"ä»»åŠ¡å¤±è´¥: é”™è¯¯ç  {error_code}, é”™è¯¯ä¿¡æ¯: {error_message}")
                        
                    # æ£€æŸ¥è¶…æ—¶
                    if time.time() - poll_start > max_wait_seconds:
                        raise Exception("ä»»åŠ¡è½®è¯¢è¶…æ—¶ï¼Œè¯·ç¨åé‡è¯•æˆ–é™ä½å¹¶å‘")
                        
                    # æœªå®Œæˆï¼Œç»§ç»­è½®è¯¢
                    time.sleep(5)
            else:
                raise Exception(f"æœªè¯†åˆ«çš„APIè¿”å›æ ¼å¼: {submission_json}")
            
            # ä¸‹è½½ç¼–è¾‘åçš„å›¾ç‰‡
            img_response = requests.get(result_image_url, timeout=30)
            if img_response.status_code != 200:
                raise Exception(f"å›¾ç‰‡ä¸‹è½½å¤±è´¥: {img_response.status_code}")
            
            # å¤„ç†å›¾ç‰‡
            pil_image = Image.open(BytesIO(img_response.content))
            if pil_image.mode != 'RGB':
                pil_image = pil_image.convert('RGB')
            
            # è½¬æ¢ä¸ºComfyUIéœ€è¦çš„æ ¼å¼
            image_np = np.array(pil_image).astype(np.float32) / 255.0
            image_tensor = torch.from_numpy(image_np)[None,]
            
            print("ğŸ‰ å›¾ç‰‡ç¼–è¾‘å®Œæˆï¼")
            return (image_tensor,)
            
        except Exception as e:
            print(f"é­”æ­API-LoRAå›¾åƒç¼–è¾‘è°ƒç”¨å¤±è´¥: {str(e)}")
            # è¿”å›åŸå›¾åƒä½œä¸ºé”™è¯¯å›é€€
            return (image.unsqueeze(0),)

# èŠ‚ç‚¹æ˜ å°„å’Œæ˜¾ç¤ºåç§°æ˜ å°„
NODE_CLASS_MAPPINGS = {
    "modelscopeLoraTextToImageNode": modelscopeLoraTextToImageNode,
    "modelscopeLoraImageEditNode": modelscopeLoraImageEditNode
}

NODE_DISPLAY_NAME_MAPPINGS = {
    "modelscopeLoraTextToImageNode": "é­”æ­API-æ–‡ç”Ÿå›¾",
    "modelscopeLoraImageEditNode": "é­”æ­API-å›¾åƒç¼–è¾‘"
}

__all__ = ['NODE_CLASS_MAPPINGS', 'NODE_DISPLAY_NAME_MAPPINGS']